# -*- coding: utf-8 -*-
"""Gado-Gado Classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XBbksS1vwMiULEjvcLZBezEi5YMaHk0U
"""

from google.colab import files
uploaded = files.upload()

import numpy as np 
import pandas as pd 
import matplotlib.pyplot as plt
import tensorflow as tf

df = pd.read_csv('Dataset Bangkit Gado-Gado.csv')
df

df = df.replace({'pm10' : {'---': np.nan},
                 'so2' : {'---': np.nan},
                 'co' : {'---': np.nan},
                 'o3' : {'---': np.nan},
                 'no2' : {'---': np.nan},
                 'max' : {1: np.nan, 4: np.nan},
                 'categori' : {'TIDAK ADA DATA': np.nan,
                               'PM10': np.nan,
                               'CO': np.nan,
                               'O3': np.nan,
                               'SO2': np.nan,
                               'BAIK': 0,
                               'SEDANG': 1,
                               'TIDAK SEHAT': 2,
                               'SANGAT TIDAK SEHAT': 2,
                               'BERBAHAYA': 2}})

df=df.dropna()
df

df.info()

df['pm10'] = pd.to_numeric(df['pm10'],errors = 'coerce')
df['so2'] = pd.to_numeric(df['so2'],errors = 'coerce')
df['co'] = pd.to_numeric(df['co'],errors = 'coerce')
df['o3'] = pd.to_numeric(df['o3'],errors = 'coerce')
df['no2'] = pd.to_numeric(df['no2'],errors = 'coerce')

df.duplicated().sum()

df.drop_duplicates(subset=["stasiun", "pm10", "so2", "co", "o3", "no2", "max", "critical"], keep=False, inplace=True)
df.drop(['tanggal', 'max', 'critical', 'stasiun'],axis='columns', inplace=True)
df

df.categori.value_counts()

# pisahkan kategori 1 dan drop dari df
df_1 = df[df['categori']==1]
df = df.drop(df[df.categori == 1].index)

# ambil 2500 data saja
df_1 = df_1.sample(n=2500)

# gabungkan kembali dengan df
df = pd.merge(df, df_1, how='outer')

df.categori.value_counts()

df.isnull().sum()

df.count()

"""# modelling"""

feature_names = ['pm10', 'so2', 'co', 'o3', 'no2']

X = np.array(df[feature_names], dtype=float)
y = np.array(df.categori, dtype=int)

df.categori.value_counts()

from keras.models import Sequential
from keras.layers import Dense
from keras.callbacks import ModelCheckpoint

# pisahin train test
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, shuffle=True, random_state=0)

model = Sequential()
model.add(Dense(15, input_dim=5, activation='relu'))
model.add(Dense(8, activation='relu'))
model.add(Dense(8, activation='relu'))
model.add(Dense(3, activation='softmax'))

model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

best_model_only = ModelCheckpoint('best_model.h5', save_best_only=True, monitor='val_accuracy', mode='max', verbose=1)

model.fit(X_train, y_train, validation_data = (X_test,y_test), epochs=500, batch_size=32, callbacks=[best_model_only])

# Recreate the exact same model, including its weights and the optimizer
new_model = tf.keras.models.load_model('best_model.h5')

# Show the model architecture
new_model.summary()

loss, acc = new_model.evaluate(X_test,y_test, verbose=2)
print('Restored model, accuracy: {:5.2f}%'.format(100 * acc))

X = np.array([89, 126, 47, 61, 104], dtype=float)[np.newaxis]
new_model.predict_classes(X)[0]
